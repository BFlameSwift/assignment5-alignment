{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ec5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Callable\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from xopen import xopen\n",
    "\n",
    "from drgrpo_grader import r1_zero_reward_fn\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_MATH_dataset(filepath, prompt_key=\"problem\", answer_key=\"solution\"):\n",
    "\tprompts = []\n",
    "\tanswers = []\n",
    "\t\n",
    "\twith xopen(filepath, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tdata = json.loads(line.strip())\n",
    "\t\t\tprompts.append(data[prompt_key])\n",
    "\t\t\tanswers.append(data[answer_key])\n",
    "\t\n",
    "\treturn prompts, answers\n",
    "def get_data_hour_str() -> str:\n",
    "\treturn datetime.now().strftime(\"%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba762f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_validation_path = '../data/MATH/validation.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_zero_prompt =\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User: {question}\n",
    "Assistant: <think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vllm( vllm_model: LLM, reward_fn: Callable[[str, str], dict[str, float]], prompts: List[str], eval_sampling_params: SamplingParams,output_path=None,solutions=None) -> None: \n",
    "    \"\"\" Evaluate a language model on a list of prompts, compute evaluation metrics, and serialize results to disk. \"\"\"\n",
    "\toutputs = vllm_model.generate(prompts, eval_sampling_params)\n",
    "\n",
    "\tif isinstance(output_path,str):\n",
    "\t\toutput_path = Path(output_path)\n",
    "\tif output_path is None:\n",
    "\t\toutput_path = Path(\"../data/output\") / (get_data_hour_str()+\".jsonl\")\n",
    "\n",
    "\treward_results = []\n",
    "\t\n",
    "\n",
    "\twith open(output_path,'w') as f:\n",
    "\t\tfor idx,output in enumerate(outputs): \n",
    "\t\t\tprompt = output.prompt \n",
    "\t\t\tgenerated_text = output.outputs[0].text \n",
    "\t\t\tjson_line = {\"prompt\":prompt,\"predict\":generated_text}\n",
    "\t\t\tf.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "\t\t\tr = reward_fn(generated_text,solutions[idx])\n",
    "\n",
    "\t\t\treward_results.append(r)\n",
    "\t\n",
    "\twith open(output_path.replace(\".jsonl\",\"_reward.json\",'w')):\n",
    "\t\tjson.dumps(reward_results,indent=4,ensure_ascll=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\"../models/Qwen2.5-Math-1.5B\")\n",
    "prompts,solutions = preprocess_MATH_dataset(MATH_validation_path)\n",
    "sampling_params = SamplingParams( temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"\\n\"] )\n",
    "evaluate_vllm(llm,prompts,r1_zero_reward_fn,prompts,sampling_params,solutions=solutions,output_path=\"../data/output/math_qwen2.5_1.5b_\"+get_data_hour_str()+\".jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
